{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuCnJHTZJgyrqa1BN2kXGe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RandiBro234/Analisis-Sentimen-with-Machine-Learning-Approach/blob/main/Analisis_Sentimen_Machine_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "eNHxYjbdQ7vz",
        "outputId": "47b345d8-e54c-4f16-ddbb-ca6bb884beea"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-18526902-7257-489d-bf69-c937562f99df\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-18526902-7257-489d-bf69-c937562f99df\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Instagram Comments Scraping.csv to Instagram Comments Scraping (2).csv\n",
            "Shape: (316, 4)\n",
            "Kolom: ['Username', 'Like_Count', 'Created_At', 'Comment']\n",
            "            Username  Like_Count        Created_At  \\\n",
            "0       jendamclover         964   10/12/2025 5:01   \n",
            "1           a.apink_           0  10/12/2025 16:34   \n",
            "2        ggfc.cepogo           0  10/12/2025 16:33   \n",
            "3      auliaasmulnst           0  10/12/2025 11:08   \n",
            "4    wong.ora.kanggo           0  10/12/2025 11:07   \n",
            "..               ...         ...               ...   \n",
            "295          m_irido           0  10/16/2025 16:46   \n",
            "296        ririn_408           0  10/16/2025 12:59   \n",
            "297      haenurtimah           0  10/16/2025 10:56   \n",
            "298        baliyau__           1   10/16/2025 5:11   \n",
            "299  ahmadfirdaus_72           2  10/15/2025 23:27   \n",
            "\n",
            "                                               Comment  \n",
            "0      Kemarin rasanya piala dunia sudah di depan mata  \n",
            "1                                           out petrik  \n",
            "2                                          #patrickout  \n",
            "3    Posisi bang? @arya.m.sinulingga kok gak nampak...  \n",
            "4    ERICK THOHIR ANJING,, KLUIVERT BABI,, BUBARKAN...  \n",
            "..                                                 ...  \n",
            "295  Stop naturalisasi, mending pakai lokal saja ka...  \n",
            "296                          Cape bangettt gegara eric  \n",
            "297                                                  ⁶  \n",
            "298  Ganti Erick tohirnya aja 🤣 dia pebisnis bukan ...  \n",
            "299  jgn nonton pertandingan timnas, klo masih di l...  \n",
            "\n",
            "[300 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "# Session 1: Upload dan lihat struktur dataset\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"Instagram Comments Scraping.csv\")\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"Kolom:\", list(df.columns))\n",
        "print(df.head(300))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Session 2: Deteksi kolom teks paling mungkin (comment/text/content)\n",
        "possible_text_cols = [c for c in df.columns if any(k in c.lower() for k in (\"comment\",\"koment\",\"text\",\"isi\",\"content\",\"caption\"))]\n",
        "if len(possible_text_cols) == 0:\n",
        "    print(\"Tidak menemukan kolom teks otomatis. Silakan ganti 'text_col' manual sesuai dataset.\")\n",
        "    print(\"Kolom tersedia:\", list(df.columns))\n",
        "    text_col = input(\"Masukkan nama kolom yang berisi komentar: \")\n",
        "else:\n",
        "    text_col = possible_text_cols[0]\n",
        "    print(\"Terpilih kolom teks:\", text_col)\n",
        "\n",
        "# Periksa beberapa contoh\n",
        "print(df[text_col].head(20))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUY5Z9TdRBfX",
        "outputId": "435569ee-9450-482a-ad26-42118cbb18a8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Terpilih kolom teks: Comment\n",
            "0       Kemarin rasanya piala dunia sudah di depan mata\n",
            "1                                            out petrik\n",
            "2                                           #patrickout\n",
            "3     Posisi bang? @arya.m.sinulingga kok gak nampak...\n",
            "4     ERICK THOHIR ANJING,, KLUIVERT BABI,, BUBARKAN...\n",
            "5     ERICK THOHIR ANJING,, KLUIVERT BABI,, BUBARKAN...\n",
            "6                  Patrik P nya PETOAKKKKK #KluivertOut\n",
            "7     Kalian HEBAT!!! Sampai disini itu sudah sangat...\n",
            "8     Info seleksi piala aff pa,manawi tiasa abi hyn...\n",
            "9                                           #patrickout\n",
            "10               Terima kasih sudah pernah sedekat ini.\n",
            "11    WWKKWKW CAPE CAPE COCH @shintaeyong7777 naikin...\n",
            "12                                          #patrickout\n",
            "13                                          #patrickout\n",
            "14             Tolong suruh angkat kaki skarang juga 🫵🏻\n",
            "15                                          #PatrickOut\n",
            "16                                          erik kontol\n",
            "17                                     RIP PATRICK !!!!\n",
            "18    Don't blame yourself guys,you did great! Maybe...\n",
            "19    pak @erickthohir sudah menghancurkan hati fans...\n",
            "Name: Comment, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Session 3: Preprocessing teks\n",
        "import re\n",
        "\n",
        "def clean_text(s):\n",
        "    s = str(s).lower()\n",
        "    s = re.sub(r'http\\S+|www\\.\\S+', ' ', s)    # hapus URL\n",
        "    s = re.sub(r'[^a-z0-9\\s]', ' ', s)        # hapus simbol (tetap angka jika perlu)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "df['clean_text'] = df[text_col].apply(clean_text)\n",
        "print(df[['clean_text']].head(300))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2g_gZtmuR0fa",
        "outputId": "edbb7ba2-ae8b-4926-d54e-182831479f92"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            clean_text\n",
            "0      kemarin rasanya piala dunia sudah di depan mata\n",
            "1                                           out petrik\n",
            "2                                           patrickout\n",
            "3    posisi bang arya m sinulingga kok gak nampak a...\n",
            "4    erick thohir anjing kluivert babi bubarkan pss...\n",
            "..                                                 ...\n",
            "295  stop naturalisasi mending pakai lokal saja kal...\n",
            "296                          cape bangettt gegara eric\n",
            "297                                                   \n",
            "298  ganti erick tohirnya aja dia pebisnis bukan or...\n",
            "299  jgn nonton pertandingan timnas klo masih di la...\n",
            "\n",
            "[300 rows x 1 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Session 4: Rule-based labeling (Indonesian lexicon sederhana)\n",
        "# NOTE: Ini lexicon minimal untuk percobaan; kamu bisa tambah kata sesuai domain/data.\n",
        "positive_words = {\n",
        "    \"bagus\",\"enak\",\"mantap\",\"suka\",\"recommended\",\"baik\",\"nyaman\",\"recommended\",\"puas\",\"bagus\",\"keren\",\"mantap\",\n",
        "    \"love\",\"lezat\",\"sempurna\",\"murah\",\"cepat\",\"great\",\"hebat\",\"luar biasa\",\"NT\",\"nice try\",\"nt\",\"terimakasih\",\"terima kasih\",\n",
        "    \"respect\"\n",
        "}\n",
        "negative_words = {\n",
        "    \"buruk\",\"jelek\",\"masalah\",\"kecewa\",\"worst\",\"parah\",\"lama\",\"mahal\",\"ga\",\"gak\",\"tidak\",\"tidaknya\",\"tidaklah\",\n",
        "    \"bangke\",\"menyebalkan\",\"bete\",\"horek\",\"anjing\",\"babi\",\"out\",\"kontol\",\"menghancurkan\", \"angkat kaki\",\"patrickout\",\"petrik\",\n",
        "    \"kluivertout\",\"patrickkluivertout\",\"gagal\",\"taek\"\n",
        "}\n",
        "\n",
        "def lexicon_score(s):\n",
        "    toks = s.split()\n",
        "    score = 0\n",
        "    for t in toks:\n",
        "        if t in positive_words: score += 1\n",
        "        if t in negative_words: score -= 1\n",
        "    return score\n",
        "\n",
        "# Hitung score dan tentukan label 3 kelas: 'pos','neu','neg'\n",
        "df['lex_score'] = df['clean_text'].apply(lexicon_score)\n",
        "def map_label(score):\n",
        "    if score > 0: return 'positive'\n",
        "    if score < 0: return 'negative'\n",
        "    return 'neutral'\n",
        "\n",
        "df['auto_sentiment'] = df['lex_score'].apply(map_label)\n",
        "\n",
        "# Tampilkan distribusi label buatan\n",
        "print(\"Distribusi label (automatis):\")\n",
        "print(df['auto_sentiment'].value_counts())\n",
        "print(df[['clean_text','lex_score','auto_sentiment']].head(300))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pEdSLnvR2qC",
        "outputId": "a678885c-9601-4bcc-c275-438c3cd79a9a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribusi label (automatis):\n",
            "auto_sentiment\n",
            "neutral     203\n",
            "negative     92\n",
            "positive     21\n",
            "Name: count, dtype: int64\n",
            "                                            clean_text  lex_score  \\\n",
            "0      kemarin rasanya piala dunia sudah di depan mata          0   \n",
            "1                                           out petrik         -2   \n",
            "2                                           patrickout         -1   \n",
            "3    posisi bang arya m sinulingga kok gak nampak a...         -1   \n",
            "4    erick thohir anjing kluivert babi bubarkan pss...         -2   \n",
            "..                                                 ...        ...   \n",
            "295  stop naturalisasi mending pakai lokal saja kal...          0   \n",
            "296                          cape bangettt gegara eric          0   \n",
            "297                                                             0   \n",
            "298  ganti erick tohirnya aja dia pebisnis bukan or...          0   \n",
            "299  jgn nonton pertandingan timnas klo masih di la...         -2   \n",
            "\n",
            "    auto_sentiment  \n",
            "0          neutral  \n",
            "1         negative  \n",
            "2         negative  \n",
            "3         negative  \n",
            "4         negative  \n",
            "..             ...  \n",
            "295        neutral  \n",
            "296        neutral  \n",
            "297        neutral  \n",
            "298        neutral  \n",
            "299       negative  \n",
            "\n",
            "[300 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Session 5: TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "texts = df['clean_text'].astype(str)\n",
        "y_auto = df['auto_sentiment'].astype(str)\n",
        "\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=2, max_df=0.95)  # unigram+bigram, filter kata jarang\n",
        "X_sparse = vectorizer.fit_transform(texts)\n",
        "\n",
        "print(\"Shape TF-IDF:\", X_sparse.shape)\n",
        "print(\"Contoh fitur:\", list(vectorizer.vocabulary_.keys())[:20])\n",
        "\n",
        "# Buat dataframe kecil untuk verifikasi\n",
        "tmp = df[['clean_text','auto_sentiment']].copy()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Er-Db5ZWaa7",
        "outputId": "d8f3d4bc-fe9c-412f-c6b9-ac0f1ef2af1c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape TF-IDF: (316, 376)\n",
            "Contoh fitur: ['piala', 'dunia', 'sudah', 'di', 'piala dunia', 'out', 'petrik', 'patrickout', 'arya', 'sinulingga', 'kok', 'gak', 'pas', 'timnas', 'kalah', 'ini', 'arya sinulingga', 'erick', 'thohir', 'anjing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Session 6: Split + SMOTE oversampling (SMOTE but needs dense arrays)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Filter untuk memastikan setiap kelas minimal 2 contoh (jika ada kelas 1 contoh sangat kecil, SMOTE gagal)\n",
        "vc = y_auto.value_counts()\n",
        "print(\"Label counts before filtering:\\n\", vc)\n",
        "\n",
        "mask = y_auto.map(vc) >= 1  # kita izinkan kelas 1 jika perlu — SMOTE memerlukan >=1? but we'll check later\n",
        "X = X_sparse\n",
        "y = y_auto.reset_index(drop=True)\n",
        "\n",
        "# Split terlebih dahulu (stratify hanya jika setiap kelas >=2)\n",
        "if (vc >= 2).all():\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "else:\n",
        "    # kalau ada kelas dengan 1 sampel saja, kita split tanpa stratify\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Train/test sizes:\", X_train.shape, X_test.shape)\n",
        "print(\"Train label distrib:\", Counter(y_train))\n",
        "print(\"Test label distrib:\", Counter(y_test))\n",
        "\n",
        "# SMOTE memerlukan array dense; dataset biasanya kecil jadi ok to_dense\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "try:\n",
        "    X_train_dense = X_train.toarray()\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_train_res, y_train_res = smote.fit_resample(X_train_dense, y_train)\n",
        "    print(\"After SMOTE distribution:\", Counter(y_train_res))\n",
        "except Exception as e:\n",
        "    # Jika SMOTE gagal (mis. karena kelas dengan 1 contoh), fallback: RandomOverSampler\n",
        "    print(\"SMOTE gagal:\", e)\n",
        "    from imblearn.over_sampling import RandomOverSampler\n",
        "    ros = RandomOverSampler(random_state=42)\n",
        "    X_train_res, y_train_res = ros.fit_resample(X_train, y_train)\n",
        "    # If ros returned sparse arrays, convert if necessary for some models\n",
        "    print(\"After RandomOverSampler distribution:\", Counter(y_train_res))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNT0m9O2WcRJ",
        "outputId": "7127da79-f6a9-4ddd-b08f-b4982ce1ec8f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label counts before filtering:\n",
            " auto_sentiment\n",
            "neutral     203\n",
            "negative     92\n",
            "positive     21\n",
            "Name: count, dtype: int64\n",
            "Train/test sizes: (252, 376) (64, 376)\n",
            "Train label distrib: Counter({'neutral': 162, 'negative': 73, 'positive': 17})\n",
            "Test label distrib: Counter({'neutral': 41, 'negative': 19, 'positive': 4})\n",
            "After SMOTE distribution: Counter({'neutral': 162, 'negative': 162, 'positive': 162})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Session 7: Training & Evaluation\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Jika X_train_res masih dense numpy karena SMOTE, kita fine; kalau sparse (ros), handle accordingly.\n",
        "# Pastikan bentuk X_test sesuai type model expect (dense/sparse). Many sklearn classifiers accept dense arrays.\n",
        "try:\n",
        "    # detect numpy array\n",
        "    _ = X_train_res.shape\n",
        "    X_train_final = X_train_res\n",
        "except:\n",
        "    X_train_final = X_train_res.toarray()\n",
        "\n",
        "# For test, convert to dense if needed (some classifiers accept sparse)\n",
        "X_test_final = X_test.toarray() if hasattr(X_test, \"toarray\") else X_test\n",
        "\n",
        "models = {\n",
        "    \"NaiveBayes\": MultinomialNB(),\n",
        "    \"LogisticRegression\": LogisticRegression(max_iter=1000),\n",
        "    \"LinearSVM\": LinearSVC(),\n",
        "    \"RandomForest\": RandomForestClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "results = []\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_final, y_train_res)\n",
        "    y_pred = model.predict(X_test_final)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred, zero_division=0, output_dict=True)\n",
        "    results.append({\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": acc,\n",
        "        \"Precision_macro\": report['macro avg']['precision'],\n",
        "        \"Recall_macro\": report['macro avg']['recall'],\n",
        "        \"F1_macro\": report['macro avg']['f1-score']\n",
        "    })\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(\"Accuracy:\", acc)\n",
        "    print(classification_report(y_test, y_pred, zero_division=0))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "df_results = pd.DataFrame(results).sort_values(by='F1_macro', ascending=False)\n",
        "print(\"\\nPerbandingan model:\")\n",
        "print(df_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAd__FQqWf5N",
        "outputId": "8d5c8620-7a63-4bc3-df6d-bdd939571e16"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== NaiveBayes ===\n",
            "Accuracy: 0.546875\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.44      0.74      0.55        19\n",
            "     neutral       0.75      0.44      0.55        41\n",
            "    positive       0.38      0.75      0.50         4\n",
            "\n",
            "    accuracy                           0.55        64\n",
            "   macro avg       0.52      0.64      0.53        64\n",
            "weighted avg       0.63      0.55      0.55        64\n",
            "\n",
            "Confusion Matrix:\n",
            " [[14  5  0]\n",
            " [18 18  5]\n",
            " [ 0  1  3]]\n",
            "\n",
            "=== LogisticRegression ===\n",
            "Accuracy: 0.890625\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.93      0.74      0.82        19\n",
            "     neutral       0.87      0.98      0.92        41\n",
            "    positive       1.00      0.75      0.86         4\n",
            "\n",
            "    accuracy                           0.89        64\n",
            "   macro avg       0.93      0.82      0.87        64\n",
            "weighted avg       0.90      0.89      0.89        64\n",
            "\n",
            "Confusion Matrix:\n",
            " [[14  5  0]\n",
            " [ 1 40  0]\n",
            " [ 0  1  3]]\n",
            "\n",
            "=== LinearSVM ===\n",
            "Accuracy: 0.875\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.88      0.74      0.80        19\n",
            "     neutral       0.87      0.95      0.91        41\n",
            "    positive       1.00      0.75      0.86         4\n",
            "\n",
            "    accuracy                           0.88        64\n",
            "   macro avg       0.91      0.81      0.85        64\n",
            "weighted avg       0.88      0.88      0.87        64\n",
            "\n",
            "Confusion Matrix:\n",
            " [[14  5  0]\n",
            " [ 2 39  0]\n",
            " [ 0  1  3]]\n",
            "\n",
            "=== RandomForest ===\n",
            "Accuracy: 0.890625\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      0.68      0.81        19\n",
            "     neutral       0.87      1.00      0.93        41\n",
            "    positive       0.75      0.75      0.75         4\n",
            "\n",
            "    accuracy                           0.89        64\n",
            "   macro avg       0.87      0.81      0.83        64\n",
            "weighted avg       0.90      0.89      0.89        64\n",
            "\n",
            "Confusion Matrix:\n",
            " [[13  5  1]\n",
            " [ 0 41  0]\n",
            " [ 0  1  3]]\n",
            "\n",
            "Perbandingan model:\n",
            "                Model  Accuracy  Precision_macro  Recall_macro  F1_macro\n",
            "1  LogisticRegression  0.890625         0.934300      0.820817  0.866737\n",
            "2           LinearSVM  0.875000         0.913889      0.812687  0.854707\n",
            "3        RandomForest  0.890625         0.874113      0.811404  0.831439\n",
            "0          NaiveBayes  0.546875         0.520833      0.641955  0.534289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Session 8: Prediksi semua data (opsional) & ringkasan distribusi sentimen (menggunakan model terbaik)\n",
        "best_model_name = df_results.iloc[0]['Model']\n",
        "best_model = models[best_model_name]\n",
        "\n",
        "# Prediksi terhadap seluruh dataset (gunakan vectorizer & best_model; pastikan format sama)\n",
        "X_all = vectorizer.transform(df['clean_text'].astype(str))\n",
        "X_all_final = X_all.toarray() if hasattr(X_all, \"toarray\") else X_all\n",
        "df['predicted_sentiment'] = best_model.predict(X_all_final)\n",
        "\n",
        "print(\"Distribusi prediksi model terbaik:\")\n",
        "print(df['predicted_sentiment'].value_counts(normalize=False))\n",
        "\n",
        "# Contoh: tampil beberapa komentar yang diprediksi negative\n",
        "print(\"\\nBeberapa contoh komentar predicted NEGATIVE:\")\n",
        "print(df[df['predicted_sentiment']=='negative'][['clean_text']].head(300))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEJDPih_Wqol",
        "outputId": "5174e8e3-7a2a-46db-b42d-1bab04fd7a96"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribusi prediksi model terbaik:\n",
            "predicted_sentiment\n",
            "neutral     208\n",
            "negative     88\n",
            "positive     20\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Beberapa contoh komentar predicted NEGATIVE:\n",
            "                                            clean_text\n",
            "1                                           out petrik\n",
            "2                                           patrickout\n",
            "4    erick thohir anjing kluivert babi bubarkan pss...\n",
            "5    erick thohir anjing kluivert babi bubarkan pss...\n",
            "6                  patrik p nya petoakkkkk kluivertout\n",
            "..                                                 ...\n",
            "301                                kluivertout aryaout\n",
            "302                                kluivertout aryaout\n",
            "305                                        kluivertout\n",
            "307                                             pk out\n",
            "313  di bangun oleh sty dihancurkan oleh petrik sem...\n",
            "\n",
            "[88 rows x 1 columns]\n"
          ]
        }
      ]
    }
  ]
}